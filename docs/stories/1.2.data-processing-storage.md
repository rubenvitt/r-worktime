# Story 1.2: Data Processing & Storage

## Status
Draft

## Story
**As a** system,  
**I want** to parse and store Timing export data with duplicate detection,  
**so that** users have a clean, consistent dataset for overtime calculations

## Acceptance Criteria

1. JSON structure from Timing exports is correctly parsed and validated
2. Time entries are stored on a daily basis in the database
3. Duplicate entries are detected based on date and time overlap
4. Overlapping time periods trigger replace logic (newer data replaces older)
5. Data validation includes plausibility checks (e.g., max 24h per day)
6. Invalid entries are logged but don't block valid data processing
7. Processing handles partial failures gracefully
8. Database transaction ensures data consistency

## Tasks / Subtasks

- [ ] Implement JSON parser for Timing format (AC: 1)
  - [ ] Create TypeScript interfaces for Timing data structure
  - [ ] Implement parsing logic with error handling
  - [ ] Validate required fields presence
  
- [ ] Design and implement database schema (AC: 2)
  - [ ] Create time_entries table with appropriate indices
  - [ ] Set up daily aggregation views
  - [ ] Implement database migrations
  
- [ ] Build duplicate detection system (AC: 3)
  - [ ] Create unique constraint on date + time range
  - [ ] Implement overlap detection algorithm
  - [ ] Add duplicate entry logging
  
- [ ] Implement replace logic for overlaps (AC: 4)
  - [ ] Detect overlapping time periods
  - [ ] Create replace transaction logic
  - [ ] Maintain audit trail of replacements
  
- [ ] Add data validation layer (AC: 5, 6)
  - [ ] Implement daily hour limit check (max 24h)
  - [ ] Validate date formats and ranges
  - [ ] Check for negative durations
  - [ ] Create validation error log table
  
- [ ] Implement error handling (AC: 7)
  - [ ] Separate valid from invalid entries
  - [ ] Process valid entries even if some fail
  - [ ] Return detailed processing report
  
- [ ] Ensure transactional integrity (AC: 8)
  - [ ] Wrap processing in database transaction
  - [ ] Implement rollback on critical failures
  - [ ] Add transaction retry logic

## Dev Notes

### Technology Stack (from Architecture)
- **Backend**: Node.js with Express
- **Database**: PostgreSQL 14+
- **ORM**: Prisma
- **Validation**: Zod for schema validation
- **Date Handling**: date-fns library

### Database Schema
```sql
-- time_entries table
id: UUID (primary key)
user_id: UUID (foreign key)
date: DATE
start_time: TIME
end_time: TIME
duration_hours: DECIMAL(4,2)
project: VARCHAR(255)
task: VARCHAR(255)
raw_data: JSONB
created_at: TIMESTAMP
updated_at: TIMESTAMP
import_batch_id: UUID

-- Unique constraint on (user_id, date, start_time, end_time)
```

### API Endpoints
- `POST /api/data/process` - Process uploaded JSON
- Response: 
```json
{
  "processed": number,
  "duplicates": number,
  "replaced": number,
  "errors": number,
  "details": []
}
```

### Validation Rules
- Maximum 24 hours per day per user
- Dates cannot be in the future
- Duration must be positive
- Start time must be before end time
- Minimum duration: 0.25 hours (15 minutes)

### Service Location
- `/src/services/dataProcessingService.ts`
- `/src/validators/timingDataValidator.ts`
- `/prisma/schema.prisma`
- `/src/db/repositories/timeEntryRepository.ts`

### Testing
- **Test Location**: `/src/__tests__/services/dataProcessing/`
- **Framework**: Jest
- **Database**: Test database with migrations
- **Test Cases Required**:
  - Valid data processing
  - Duplicate detection
  - Overlap replacement
  - Validation failures
  - Transaction rollback
  - Partial failure handling

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-11 | 1.0 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
_To be filled by development agent_

### Debug Log References
_To be filled during development_

### Completion Notes List
_To be filled during development_

### File List
_To be filled during development_

## QA Results
_To be filled by QA agent_